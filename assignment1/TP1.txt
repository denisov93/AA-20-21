Attention:
- Do not edit this file in text editors like Word. Use a plain text editor only. In case of doubt, you can use Spyder as a text editor.
- Do not change the structure of this file. Just fill in your answers in the places provided (After the R#: tag).
- You can add lines in the spaces for your answers but your answers should be brief and straight to the point.

QUESTIONS:

Q1: Considering the data provided, explain the need to standardize the attribute values.
R1: We standardize to prevent having data with different scales and to be able compare them with other data sets.


Q2: Explain how you calculated the parameters for standardization and how you used them in the test set.
R2: First we shuffle the input data then create new values from the train set subtracting by the mean then dividing by the standard deviation, both mean and standard deviation obtained from train set will be applied to the fitted data using them on the validation and test sets.


Q3: Explain how you calculated the prior probability of an example belonging to a class (the probability before taking into account the attribute values ​​of the example) in your Naïve Bayes classifier implementation. You may include a relevant piece of your code if this helps you explain.
R3: The prior probability was calculated in the Bayes method by separating data into two classes for each with the Logarithm of specific note class divided by the total of all notes, for example, p0 = (log n0/(n0+n1)) produces prior probability of class n0.


Q4: Explain how your Naïve Bayes classifier predicts the class to which a test example belongs. You may include a relevant piece of your code if this helps you explain.
R4: The estimation of each class was determined a priori after obtaining the sum probabilities of the score_samples. For each class and feature we estimate the value based on sum of previous estimatives.


Q5: Explain the effect of the bandwidth parameter on your classifier.
R5: The bandwidth greatly affects the classifier results, greater value will cause overfitting results closer to train data while smaller values are prone higher error rates leading to worse results, hence the need to find the best value for the data.


Q6: Explain what effect the C parameter has on the Logistic Regression classifier.
R6: The C parameter is a regularization value, smaller C values will cause bigger effect of the regularization, a good C value will reduce overfitting and lower validation error on a new set.


Q7: Explain how you determined the best bandwidth and C parameters for your classifier and the Logistic Regression classifier. You may include a relevant piece of your code if this helps you explain.
R7: We loop values for "bandwidth" from 0.02 to 0.6 with a step of 0.02 and "C" from 1e-3 through 1e-12, the objective is to obtain the validation with least error observed.


Q8: Explain how you obtained the best hypothesis for each classifier after optimizing all parameters.
R8: We compared using approximate normal test and McNemar's test with 95% confidence, the approximate normal test results show errors and their deviation where Naïve Bayes was prone to less validation errors then other estimators, McNemar Test show there was a difference between all estimators.


Q9: Show the best parameters, the estimate of the true error for each hypothesis you obtained (your classifier and the two provided by the library), the ranges in the expected number of errors given by the approximate normal test, the McNemar test values, and discuss what you can conclude from this.
R9: C: 0.1, Bandwidth: 0.18, Logistic Regression error: 0.11, Gaussian error: 0.12998, Naïve Bayes error: 0.08453, Logistic Regression true error: 0.00312, Gaussian true error: 0.0055, Naïve Bayes true error: 0.000023, Aprox Normal Test or LR: 138.0 ± 3.868, GS: 163.0 ± 5.133, NB: 106.0 ± 0.332, McNemar Tests: NB vs LR: 10.45, LR vs GS: 10.47, GS vs NB: 30.45,
Overall by the results obtained from the data we conclude that Gaussian had the most errors, the logistic regression had simmilar errors then Gaussian, Naïve Bayes was the best estimator for the provided data with the lowest true error, McNemar scores had values bigger then 3.84 which means all estimators producing significantly different values thus not having the same performace, biggest difference was by GS vs NB with 30.45.


